{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#wandb.init(project=\"blackjackv2\", entity = \"rl_proj\")\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","from collections import defaultdict\n","import sys\n","import numpy as np\n","from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from utils_blackjack import *\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["env = gym.make(\"Blackjack-v1\", natural=True, sab=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize counters for statistics\n","total_episodes = 100000\n","total_reward = 0\n","total_wins = 0\n","total_natural_wins = 0\n","total_losses = 0\n","total_draws = 0\n","\n","# Iterate over a specified number of episodes\n","for episode in range(total_episodes):\n","    # Reset the environment and obtain the initial observation\n","    observation = env.reset()[0]\n","    episode_reward = 0\n","    \n","    # Iterate over a maximum of 100 time steps in the episode\n","    for t in range(100):\n","        # Print the current observation\n","        print(f\"Observation: {observation}\")\n","\n","        # Check if the current observation is a natural win (e.g., blackjack score of 21)\n","        if observation[0] == 21:\n","            total_natural_wins += 1\n","        \n","        # Use the basic_policy function to determine the action\n","        action = basic_policy(observation)            \n","        print(f\"Taking action: {action}\")\n","\n","        # Take the selected action and obtain the new observation, reward, and other information\n","        observation, reward, done, term, info = env.step(action)\n","        episode_reward += reward\n","\n","        # Check if the episode is done (e.g., player's turn ends or game is finished)\n","        if done:\n","            # Print information about the end of the game and the episode's total reward\n","            print(f\"Game ended! Reward: {episode_reward}\")\n","\n","            # Accumulate total reward and update statistics\n","            total_reward += episode_reward\n","            if episode_reward > 0:\n","                total_wins += 1\n","            elif episode_reward < 0:\n","                total_losses += 1\n","            else:\n","                total_draws += 1\n","\n","            print('You won :)\\n') if episode_reward > 0 else print('You lost :(\\n')\n","            \n","            break\n","\n","# Print overall statistics after all episodes are completed\n","print(f\"Total Accumulated Reward: {total_reward:.3f}\")\n","print(f\"Percentage of Wins: {(total_wins / total_episodes * 100):.3f}%\")\n","print(f\"Percentage of Natural Wins: {(total_natural_wins / total_episodes * 100):.3f}%\")\n","print(f\"Percentage of Losses: {(total_losses / total_episodes * 100):.3f}%\")\n","print(f\"Percentage of Draws: {(total_draws / total_episodes * 100):.3f}%\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### MONTECARLO"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["num_episodes = 300000\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#running the monte carlo method\n","policy2, Q , returns, epsilons = mc_control(env, num_episodes, 0.02)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Just for understanding processes\n","print (policy2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Run the agent with the policy obtained during the train\n","run_agentmc(policy2, env)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Printing the shape of the policy to compare with the optimal one\n","plot_policy(policy2)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":2}
