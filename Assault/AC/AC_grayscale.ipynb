{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = filter out INFO, 2 = filter out WARNING, 3 = filter out ERROR\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import logging\n",
    "gym_logger = logging.getLogger('gym')\n",
    "gym_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"ActorCritic grayscale\", entity = \"rl_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import namedtuple, deque\n",
    "from ale_py import ALEInterface\n",
    "import wandb\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "ale = ALEInterface()\n",
    "\n",
    "class Config:\n",
    "    EPSILON_START = 1.0\n",
    "    EPSILON_END = 0.01\n",
    "    EPSILON_DECAY_RATE = 0.99\n",
    "    EPISODES = 30  \n",
    "    BATCH_SIZE = 128\n",
    "    GAMMA = 0.999\n",
    "    MAX_STEPS_PER_EPISODE = 1000\n",
    "    LEARNING_RATE = 1e-4 \n",
    "    MEMORY_SIZE = 10000\n",
    "\n",
    "config = Config()\n",
    "best_reward = 0\n",
    "\n",
    "env = gym.make(\"Assault-v4\", render_mode=\"rgb_array\")\n",
    "#env = gym.make(\"Assault-v4\") \n",
    "n_actions = env.action_space.n\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(self.transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "class Actor(keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        #self.conv1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")\n",
    "        self.conv1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\", kernel_initializer='he_normal')\n",
    "        #self.conv2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")\n",
    "        self.conv2 = layers.Conv2D(64, 4, strides=2, activation=None)  # Remove activation here\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.d1 = layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))\n",
    "        self.d2 = layers.Dense(n_actions, activation=\"softmax\")  # Output layer for action probabilities\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        #x = self.conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.nn.relu(self.batch_norm1(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "class Critic(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")\n",
    "        self.conv2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")\n",
    "        self.conv3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.d1 = layers.Dense(512, activation=\"relu\")\n",
    "        self.d2 = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "actor_model = Actor(n_actions)\n",
    "\n",
    "critic_model = Critic()\n",
    "\n",
    "def rgb_to_grayscale(rgb):\n",
    "    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "actor_model.build(input_shape=(None, 210, 160, 1))  # 1 channel for grayscale\n",
    "critic_model.build(input_shape=(None, 210, 160, 1))\n",
    "\n",
    "dummy_input = np.random.random((1, 210, 160, 1))  # Updated for grayscale\n",
    "actor_model(dummy_input)\n",
    "critic_model(dummy_input)\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "actor_optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "critic_optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "memory = ReplayMemory(config.MEMORY_SIZE)\n",
    "\n",
    "\n",
    "def take_action(state, epsilon):\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        action_probabilities = actor_model.predict(state)\n",
    "\n",
    "        if np.isnan(action_probabilities).any():\n",
    "            return env.action_space.sample() \n",
    "        return np.random.choice(n_actions, p=np.squeeze(action_probabilities))\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < config.BATCH_SIZE:\n",
    "        return  # Exit the function if not enough samples\n",
    "\n",
    "    # Sample a batch of transitions from the replay memory\n",
    "    transitions = memory.sample(config.BATCH_SIZE)\n",
    "    batch = memory.transition(*zip(*transitions))\n",
    "\n",
    "    # Convert the batches into numpy arrays for processing\n",
    "    state_batch = np.array(batch.state).reshape(-1, 210, 160, 1)\n",
    "    \n",
    "    action_batch = np.array(batch.action)\n",
    "    reward_batch = np.array(batch.reward)\n",
    "    next_state_batch = np.array(batch.next_state).reshape(-1, 210, 160, 1)\n",
    "    \n",
    "    done_batch = np.array(batch.done)\n",
    "\n",
    "    # Critic Update\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the values from the Critic model\n",
    "        values = critic_model(state_batch)\n",
    "        # Create a dummy target for simplicity\n",
    "        dummy_target = tf.random.uniform(shape=values.shape)\n",
    "        # Compute a simple mean squared error\n",
    "\n",
    "        values_squeezed = tf.squeeze(values)\n",
    "        if len(values_squeezed.shape) > 1:\n",
    "            raise ValueError(\"Critic model's output is not a 1D array\")\n",
    "\n",
    "        critic_loss = tf.math.reduce_mean(tf.math.square(dummy_target - values))\n",
    "\n",
    "    critic_grads = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "    critic_grads, _ = tf.clip_by_global_norm(critic_grads, 1.0)  # Gradient clipping\n",
    "    critic_optimizer.apply_gradients(zip(critic_grads, critic_model.trainable_variables))\n",
    "\n",
    "    # Debugging: Print gradients and corresponding variables\n",
    "    for grad, var in zip(critic_grads, critic_model.trainable_variables):\n",
    "        if grad is None:\n",
    "            print(f\"Gradient is None for variable {var.name}\")\n",
    "\n",
    "    # Filter out None gradients\n",
    "    critic_grads_and_vars = [(grad, var) for grad, var in zip(critic_grads, critic_model.trainable_variables) if grad is not None]\n",
    "\n",
    "    # Apply gradients if there are valid ones\n",
    "    if critic_grads_and_vars:\n",
    "        critic_optimizer.apply_gradients(critic_grads_and_vars)\n",
    "    else:\n",
    "        print(\"No valid gradients to apply.\")\n",
    "\n",
    "    # Actor Update\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict the action probabilities for the current state\n",
    "        action_probs = actor_model(state_batch)\n",
    "        # Create a one-hot encoded mask for the taken actions\n",
    "        action_mask = tf.one_hot(action_batch, n_actions)\n",
    "        # Select the probabilities for the actions that were actually taken\n",
    "        selected_action_probs = tf.reduce_sum(action_probs * action_mask, axis=1)\n",
    "\n",
    "        # Adjust dummy target values shape to match the values\n",
    "        dummy_target_values = np.zeros_like(values_squeezed.numpy())\n",
    "        advantage = dummy_target_values - values_squeezed\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        actor_loss = -tf.math.reduce_mean(tf.math.log(selected_action_probs + epsilon) * advantage)\n",
    "\n",
    "    actor_grads = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "    actor_grads, _ = tf.clip_by_global_norm(actor_grads, 0.5)  # Gradient clipping\n",
    "    actor_optimizer.apply_gradients(zip(actor_grads, actor_model.trainable_variables))\n",
    "    for grad in actor_grads:\n",
    "        if tf.reduce_any(tf.math.is_inf(grad)).numpy() or tf.reduce_any(tf.math.is_nan(grad)).numpy():\n",
    "            print(\"Inf or NaN detected in actor gradients\")\n",
    "\n",
    "episode_rewards = []\n",
    "epsilon = config.EPSILON_START\n",
    "\n",
    "for episode in range(config.EPISODES):\n",
    "    state, info = env.reset()\n",
    "\n",
    "    state = rgb_to_grayscale(state) / 255.0\n",
    "    state = state.reshape(1, 210, 160, 1)  # Correctly reshape\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    info ={'lives': 4, 'episode_frame_number': 2, 'frame_number': 2}\n",
    "    frames = []\n",
    "    \n",
    "\n",
    "    while not done and info.get(\"lives\") > 0: #steps < config.MAX_STEPS_PER_EPISODE and info.get(\"lives\") >= 0:\n",
    "        action = take_action(state, epsilon)\n",
    "        \n",
    "        step_result = env.step(action)\n",
    "        \n",
    "        next_state, reward, done,_, info = step_result[:5]\n",
    "\n",
    "        next_state = rgb_to_grayscale(next_state) / 255.0\n",
    "        next_state = next_state.reshape(1, 210, 160, 1)  \n",
    "\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        optimize_model()\n",
    "\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward != 0:\n",
    "            print(\"step: \", steps, \"action: \", action, \" reward: \", reward)\n",
    "            print(\"Lives: \", info.get(\"lives\"))\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    print(f\"Episode: {episode+1}, Reward: {episode_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "\n",
    "        actor_model.save(\"./best_actor_model\", save_format=\"tf\")\n",
    "        \n",
    "        critic_model.save(\"./best_critic_model\", save_format=\"tf\")\n",
    "        print(\"New best model saved with reward:\", episode_reward)\n",
    "\n",
    "        gif_path = f\"./episode_{episode+1}_reward_{episode_reward}.gif\"\n",
    "        \n",
    "        imageio.mimsave(gif_path, frames, duration=20)\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    epsilon = max((epsilon * config.EPSILON_DECAY_RATE), config.EPSILON_END)\n",
    "\n",
    "    # Log episode metrics and GIF to wandb\n",
    "    wandb.log({\"episode\": episode + 1, \"reward\": episode_reward, \"epsilon\": epsilon, \"episode_gif\": wandb.Video(gif_path, fps=4, format=\"gif\")})\n",
    "\n",
    "env.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Rewards per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plot_path = \"./rewards_plot.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "wandb.log({\"Training process of Actor-Critic\": wandb.Image(plot_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TEST THE SAVED MODEL\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "actor_model = keras.models.load_model(\"./best_actor_model\")\n",
    "\n",
    "critic_model = keras.models.load_model(\"./best_critic_model\")\n",
    "\n",
    "env = gym.make(\"Assault-v4\", render_mode = \"rgb_array\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def take_action(state):\n",
    "    action_probabilities = actor_model.predict(state)\n",
    "    return np.random.choice(n_actions, p=np.squeeze(action_probabilities))\n",
    "\n",
    "\n",
    "rewards_per_episode = []\n",
    "best_reward = 0\n",
    "\n",
    "for episode in range(30):\n",
    "    state, info= env.reset()\n",
    "    #state = state / 255.0 \n",
    "    state = rgb_to_grayscale(state) / 255.0\n",
    "    state = state.reshape(1, 210, 160, 1)  # Correctly reshape\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    frames = []\n",
    "    info ={'lives': 4, 'episode_frame_number': 2, 'frame_number': 2}\n",
    "\n",
    "    while not done:\n",
    "        action = take_action(state)\n",
    "        next_state, reward, done,_, info = env.step(action)[:5]\n",
    "        \n",
    "        next_state = rgb_to_grayscale(next_state) / 255.0\n",
    "        next_state = next_state.reshape(1, 210, 160, 1)  # Correctly reshape\n",
    "        #next_state = next_state / 255.0  # Normaliza los valores de pÃ­xeles\n",
    "\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward != 0:\n",
    "            print(\"action: \", action, \" reward: \", reward)\n",
    "            print(\"Lives: \", info.get(\"ale.lives\"))\n",
    "\n",
    "    rewards_per_episode.append(episode_reward)\n",
    "    print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward  \n",
    "        \n",
    "        gif_path = f\"./test_episode_{episode+1}_reward_{episode_reward}.gif\"\n",
    "        imageio.mimsave(gif_path, frames, fps=30)  \n",
    "\n",
    "    # Log episode metrics and GIF to wandb\n",
    "    wandb.log({\"episode\": episode + 1, \"reward\": episode_reward, \"epsilon\": epsilon, \"episode_gif\": wandb.Video(gif_path, fps=4, format=\"gif\")})\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Grafica las recompensas por episodio\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.show()\n",
    "\n",
    "plot_path = \"./test_rewards_plot.png\"\n",
    "plt.savefig(plot_path)\n",
    "\n",
    "\n",
    "wandb.log({\"Testing of Actor-Critic\": wandb.Image(plot_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
