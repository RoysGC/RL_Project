{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = filter out INFO, 2 = filter out WARNING, 3 = filter out ERROR\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import logging\n",
    "gym_logger = logging.getLogger('gym')\n",
    "gym_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mroysgc\u001b[0m (\u001b[33mrl_proj\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/roy/Desktop/Coding/RL_Project/Assault/AC/wandb/run-20231216_195428-xelhyr6h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rl_proj/ActorCritic%20grayscale/runs/xelhyr6h' target=\"_blank\">good-cloud-1</a></strong> to <a href='https://wandb.ai/rl_proj/ActorCritic%20grayscale' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rl_proj/ActorCritic%20grayscale' target=\"_blank\">https://wandb.ai/rl_proj/ActorCritic%20grayscale</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rl_proj/ActorCritic%20grayscale/runs/xelhyr6h' target=\"_blank\">https://wandb.ai/rl_proj/ActorCritic%20grayscale/runs/xelhyr6h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rl_proj/ActorCritic%20grayscale/runs/xelhyr6h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9c54846700>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"ActorCritic grayscale\", entity = \"rl_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  68 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "step:  99 action:  1  reward:  21.0\n",
      "Lives:  3\n",
      "step:  154 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "step:  290 action:  3  reward:  21.0\n",
      "Lives:  2\n",
      "step:  320 action:  4  reward:  21.0\n",
      "Lives:  2\n",
      "step:  343 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "step:  451 action:  4  reward:  21.0\n",
      "Lives:  1\n",
      "step:  459 action:  4  reward:  21.0\n",
      "Lives:  1\n",
      "step:  494 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "Episode: 1, Reward: 189.0, Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with reward: 189.0\n",
      "step:  34 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "step:  52 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "step:  89 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  96 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "step:  131 action:  1  reward:  21.0\n",
      "Lives:  3\n",
      "step:  146 action:  0  reward:  21.0\n",
      "Lives:  3\n",
      "step:  199 action:  6  reward:  21.0\n",
      "Lives:  3\n",
      "step:  279 action:  0  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "step:  317 action:  5  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "step:  368 action:  1  reward:  21.0\n",
      "Lives:  1\n",
      "Episode: 2, Reward: 210.0, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with reward: 210.0\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  19 action:  0  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "step:  60 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "step:  124 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  158 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "step:  174 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  300 action:  5  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  644 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "step:  662 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "step:  668 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Episode: 3, Reward: 189.0, Epsilon: 0.9801\n",
      "step:  20 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  98 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  145 action:  0  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  159 action:  0  reward:  21.0\n",
      "Lives:  4\n",
      "step:  184 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  281 action:  5  reward:  21.0\n",
      "Lives:  2\n",
      "step:  309 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "step:  325 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  348 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  514 action:  0  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "step:  565 action:  4  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "step:  583 action:  4  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  617 action:  4  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Episode: 4, Reward: 273.0, Epsilon: 0.9702989999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with reward: 273.0\n",
      "step:  14 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "step:  22 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "step:  31 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  92 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  141 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "step:  280 action:  3  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  322 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  341 action:  5  reward:  21.0\n",
      "Lives:  3\n",
      "step:  364 action:  1  reward:  21.0\n",
      "Lives:  3\n",
      "step:  382 action:  6  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  508 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "step:  552 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "step:  584 action:  4  reward:  21.0\n",
      "Lives:  2\n",
      "step:  596 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "step:  617 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  673 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "step:  744 action:  1  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  822 action:  1  reward:  21.0\n",
      "Lives:  1\n",
      "step:  859 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "Episode: 5, Reward: 399.0, Epsilon: 0.96059601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with reward: 399.0\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "step:  96 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  125 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  142 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "step:  172 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "step:  196 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "step:  236 action:  3  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  335 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  386 action:  3  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  431 action:  4  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  494 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "step:  599 action:  0  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  709 action:  3  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  766 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "step:  773 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  836 action:  4  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Episode: 6, Reward: 315.0, Epsilon: 0.9509900498999999\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  20 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  100 action:  0  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  117 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "step:  139 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  197 action:  0  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  216 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "step:  346 action:  0  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  371 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  493 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  545 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  656 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  685 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "step:  705 action:  0  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  742 action:  3  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  762 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  812 action:  6  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Episode: 7, Reward: 336.0, Epsilon: 0.9414801494009999\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  61 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  103 action:  0  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  133 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "step:  164 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "step:  189 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "step:  253 action:  4  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  288 action:  4  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "step:  328 action:  6  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  419 action:  4  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  492 action:  5  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  561 action:  4  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "step:  613 action:  4  reward:  21.0\n",
      "Lives:  2\n",
      "step:  626 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  716 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "step:  722 action:  6  reward:  21.0\n",
      "Lives:  1\n",
      "step:  735 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  746 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "step:  783 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  802 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  822 action:  6  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  900 action:  1  reward:  21.0\n",
      "Lives:  1\n",
      "Episode: 8, Reward: 441.0, Epsilon: 0.9320653479069899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with reward: 441.0\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "step:  21 action:  3  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  81 action:  3  reward:  21.0\n",
      "Lives:  4\n",
      "step:  103 action:  0  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  150 action:  5  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  172 action:  3  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  235 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  356 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  393 action:  3  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "step:  405 action:  5  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  564 action:  3  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "step:  604 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  676 action:  3  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Episode: 9, Reward: 252.0, Epsilon: 0.92274469442792\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "step:  14 action:  5  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  89 action:  0  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  139 action:  1  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "step:  190 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  264 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "step:  272 action:  5  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "step:  322 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "step:  328 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  511 action:  0  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "step:  520 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "step:  585 action:  1  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  593 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "step:  629 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Episode: 10, Reward: 273.0, Epsilon: 0.9135172474836407\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  52 action:  6  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "step:  76 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  88 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  132 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "step:  149 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  210 action:  6  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  236 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  259 action:  0  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  319 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  374 action:  4  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "step:  466 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "step:  473 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  542 action:  5  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "step:  597 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  615 action:  5  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Episode: 11, Reward: 315.0, Epsilon: 0.9043820750088043\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  57 action:  0  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  100 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  136 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  210 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  302 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  322 action:  0  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "step:  373 action:  1  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "step:  419 action:  4  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  429 action:  1  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  438 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  513 action:  3  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  591 action:  0  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "step:  616 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "step:  652 action:  0  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Episode: 12, Reward: 294.0, Epsilon: 0.8953382542587163\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "step:  51 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  71 action:  5  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  113 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  155 action:  1  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  191 action:  2  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  235 action:  4  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  271 action:  6  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  281 action:  2  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  443 action:  1  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  460 action:  5  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "step:  524 action:  6  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "step:  578 action:  0  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "step:  606 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "step:  659 action:  2  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  691 action:  0  reward:  21.0\n",
      "Lives:  1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Episode: 13, Reward: 315.0, Epsilon: 0.8863848717161291\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  80 action:  3  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  149 action:  4  reward:  21.0\n",
      "Lives:  4\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "step:  249 action:  6  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "step:  328 action:  3  reward:  21.0\n",
      "Lives:  3\n",
      "step:  335 action:  5  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "step:  366 action:  3  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "step:  411 action:  1  reward:  21.0\n",
      "Lives:  3\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "step:  437 action:  2  reward:  21.0\n",
      "Lives:  2\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import namedtuple, deque\n",
    "from ale_py import ALEInterface\n",
    "import wandb\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "ale = ALEInterface()\n",
    "\n",
    "class Config:\n",
    "    EPSILON_START = 1.0\n",
    "    EPSILON_END = 0.01\n",
    "    EPSILON_DECAY_RATE = 0.99\n",
    "    EPISODES = 50  \n",
    "    BATCH_SIZE = 128\n",
    "    GAMMA = 0.999\n",
    "    MAX_STEPS_PER_EPISODE = 1000\n",
    "    LEARNING_RATE = 1e-4 \n",
    "    MEMORY_SIZE = 10000\n",
    "\n",
    "config = Config()\n",
    "best_reward = 0\n",
    "\n",
    "#env = gym.make(\"Assault-v4\", render_mode=\"rgb_array\")\n",
    "env = gym.make(\"Assault-v4\") \n",
    "n_actions = env.action_space.n\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(self.transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "class Actor(keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        #self.conv1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")\n",
    "        self.conv1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\", kernel_initializer='he_normal')\n",
    "        #self.conv2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")\n",
    "        self.conv2 = layers.Conv2D(64, 4, strides=2, activation=None)  # Remove activation here\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.d1 = layers.Dense(512, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))\n",
    "        self.d2 = layers.Dense(n_actions, activation=\"softmax\")  # Output layer for action probabilities\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        #x = self.conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.nn.relu(self.batch_norm1(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "class Critic(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")\n",
    "        self.conv2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")\n",
    "        self.conv3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.d1 = layers.Dense(512, activation=\"relu\")\n",
    "        self.d2 = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "actor_model = Actor(n_actions)\n",
    "\n",
    "critic_model = Critic()\n",
    "\n",
    "def rgb_to_grayscale(rgb):\n",
    "    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "actor_model.build(input_shape=(None, 210, 160, 1))  # 1 channel for grayscale\n",
    "critic_model.build(input_shape=(None, 210, 160, 1))\n",
    "\n",
    "dummy_input = np.random.random((1, 210, 160, 1))  # Updated for grayscale\n",
    "actor_model(dummy_input)\n",
    "critic_model(dummy_input)\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "actor_optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "critic_optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "memory = ReplayMemory(config.MEMORY_SIZE)\n",
    "\n",
    "\n",
    "def take_action(state, epsilon):\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        action_probabilities = actor_model.predict(state)\n",
    "\n",
    "        if np.isnan(action_probabilities).any():\n",
    "            return env.action_space.sample() \n",
    "        return np.random.choice(n_actions, p=np.squeeze(action_probabilities))\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < config.BATCH_SIZE:\n",
    "        return  # Exit the function if not enough samples\n",
    "\n",
    "    # Sample a batch of transitions from the replay memory\n",
    "    transitions = memory.sample(config.BATCH_SIZE)\n",
    "    batch = memory.transition(*zip(*transitions))\n",
    "\n",
    "    # Convert the batches into numpy arrays for processing\n",
    "    state_batch = np.array(batch.state).reshape(-1, 210, 160, 1)\n",
    "    \n",
    "    action_batch = np.array(batch.action)\n",
    "    reward_batch = np.array(batch.reward)\n",
    "    next_state_batch = np.array(batch.next_state).reshape(-1, 210, 160, 1)\n",
    "    \n",
    "    done_batch = np.array(batch.done)\n",
    "\n",
    "    # Critic Update\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the values from the Critic model\n",
    "        values = critic_model(state_batch)\n",
    "        # Create a dummy target for simplicity\n",
    "        dummy_target = tf.random.uniform(shape=values.shape)\n",
    "        # Compute a simple mean squared error\n",
    "\n",
    "        values_squeezed = tf.squeeze(values)\n",
    "        if len(values_squeezed.shape) > 1:\n",
    "            raise ValueError(\"Critic model's output is not a 1D array\")\n",
    "\n",
    "        critic_loss = tf.math.reduce_mean(tf.math.square(dummy_target - values))\n",
    "\n",
    "    critic_grads = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "    critic_grads, _ = tf.clip_by_global_norm(critic_grads, 1.0)  # Gradient clipping\n",
    "    critic_optimizer.apply_gradients(zip(critic_grads, critic_model.trainable_variables))\n",
    "\n",
    "    # Debugging: Print gradients and corresponding variables\n",
    "    for grad, var in zip(critic_grads, critic_model.trainable_variables):\n",
    "        if grad is None:\n",
    "            print(f\"Gradient is None for variable {var.name}\")\n",
    "\n",
    "    # Filter out None gradients\n",
    "    critic_grads_and_vars = [(grad, var) for grad, var in zip(critic_grads, critic_model.trainable_variables) if grad is not None]\n",
    "\n",
    "    # Apply gradients if there are valid ones\n",
    "    if critic_grads_and_vars:\n",
    "        critic_optimizer.apply_gradients(critic_grads_and_vars)\n",
    "    else:\n",
    "        print(\"No valid gradients to apply.\")\n",
    "\n",
    "    # Actor Update\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict the action probabilities for the current state\n",
    "        action_probs = actor_model(state_batch)\n",
    "        # Create a one-hot encoded mask for the taken actions\n",
    "        action_mask = tf.one_hot(action_batch, n_actions)\n",
    "        # Select the probabilities for the actions that were actually taken\n",
    "        selected_action_probs = tf.reduce_sum(action_probs * action_mask, axis=1)\n",
    "\n",
    "        # Adjust dummy target values shape to match the values\n",
    "        dummy_target_values = np.zeros_like(values_squeezed.numpy())\n",
    "        advantage = dummy_target_values - values_squeezed\n",
    "\n",
    "        epsilon = 1e-8\n",
    "        actor_loss = -tf.math.reduce_mean(tf.math.log(selected_action_probs + epsilon) * advantage)\n",
    "\n",
    "    actor_grads = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "    actor_grads, _ = tf.clip_by_global_norm(actor_grads, 0.5)  # Gradient clipping\n",
    "    actor_optimizer.apply_gradients(zip(actor_grads, actor_model.trainable_variables))\n",
    "    for grad in actor_grads:\n",
    "        if tf.reduce_any(tf.math.is_inf(grad)).numpy() or tf.reduce_any(tf.math.is_nan(grad)).numpy():\n",
    "            print(\"Inf or NaN detected in actor gradients\")\n",
    "\n",
    "episode_rewards = []\n",
    "epsilon = config.EPSILON_START\n",
    "\n",
    "for episode in range(config.EPISODES):\n",
    "    state= env.reset()\n",
    "\n",
    "    state = rgb_to_grayscale(state) / 255.0\n",
    "    state = state.reshape(1, 210, 160, 1)  # Correctly reshape\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    info ={'ale.lives': 4, 'episode_frame_number': 2, 'frame_number': 2}\n",
    "    frames = []\n",
    "    \n",
    "\n",
    "    while not done and info.get(\"ale.lives\") >= 0: #steps < config.MAX_STEPS_PER_EPISODE and info.get(\"lives\") >= 0:\n",
    "        action = take_action(state, epsilon)\n",
    "        \n",
    "        step_result = env.step(action)\n",
    "        \n",
    "        next_state, reward, done, info = step_result\n",
    "\n",
    "        next_state = rgb_to_grayscale(next_state) / 255.0\n",
    "        next_state = next_state.reshape(1, 210, 160, 1)  \n",
    "\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        optimize_model()\n",
    "\n",
    "        frame = env.render(mode=\"rgb_array\")\n",
    "        frames.append(frame)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward != 0:\n",
    "            print(\"step: \", steps, \"action: \", action, \" reward: \", reward)\n",
    "            print(\"Lives: \", info.get(\"ale.lives\"))\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    print(f\"Episode: {episode+1}, Reward: {episode_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "\n",
    "        actor_model.save(\"./best_actor_model\", save_format=\"tf\")\n",
    "        \n",
    "        critic_model.save(\"./best_critic_model\", save_format=\"tf\")\n",
    "        print(\"New best model saved with reward:\", episode_reward)\n",
    "\n",
    "        gif_path = f\"./episode_{episode+1}_reward_{episode_reward}.gif\"\n",
    "        \n",
    "        imageio.mimsave(gif_path, frames, duration=20)\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    epsilon = max((epsilon * config.EPSILON_DECAY_RATE), config.EPSILON_END)\n",
    "\n",
    "    # Log episode metrics and GIF to wandb\n",
    "    wandb.log({\"episode\": episode + 1, \"reward\": episode_reward, \"epsilon\": epsilon, \"episode_gif\": wandb.Video(gif_path, fps=4, format=\"gif\")})\n",
    "\n",
    "env.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_rewards)\n",
    "plt.title(\"Rewards per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plot_path = \"./rewards_plot.png\"\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "wandb.log({\"Training process of Actor-Critic\": wandb.Image(plot_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TEST THE SAVED MODEL\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "actor_model = keras.models.load_model(\"./best_actor_model\")\n",
    "\n",
    "critic_model = keras.models.load_model(\"./best_critic_model\")\n",
    "\n",
    "env = gym.make(\"Assault-v4\", render_mode = \"rgb_array\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def take_action(state):\n",
    "    action_probabilities = actor_model.predict(state)\n",
    "    return np.random.choice(n_actions, p=np.squeeze(action_probabilities))\n",
    "\n",
    "\n",
    "rewards_per_episode = []\n",
    "best_reward = 0\n",
    "\n",
    "for episode in range(50):\n",
    "    state, info= env.reset()\n",
    "    #state = state / 255.0 \n",
    "    state = rgb_to_grayscale(state) / 255.0\n",
    "    state = state.reshape(1, 210, 160, 1)  # Correctly reshape\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    frames = []\n",
    "    info ={'lives': 4, 'episode_frame_number': 2, 'frame_number': 2}\n",
    "\n",
    "    while not done:\n",
    "        action = take_action(state)\n",
    "        next_state, reward, done,_, info = env.step(action)[:5]\n",
    "        \n",
    "        next_state = rgb_to_grayscale(next_state) / 255.0\n",
    "        next_state = next_state.reshape(1, 210, 160, 1)  # Correctly reshape\n",
    "        #next_state = next_state / 255.0  # Normaliza los valores de píxeles\n",
    "\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward != 0:\n",
    "            print(\"action: \", action, \" reward: \", reward)\n",
    "            print(\"Lives: \", info.get(\"ale.lives\"))\n",
    "\n",
    "    rewards_per_episode.append(episode_reward)\n",
    "    print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward  \n",
    "        \n",
    "        gif_path = f\"./test_episode_{episode+1}_reward_{episode_reward}.gif\"\n",
    "        imageio.mimsave(gif_path, frames, fps=30)  \n",
    "\n",
    "    # Log episode metrics and GIF to wandb\n",
    "    wandb.log({\"episode\": episode + 1, \"reward\": episode_reward, \"epsilon\": epsilon, \"episode_gif\": wandb.Video(gif_path, fps=4, format=\"gif\")})\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Grafica las recompensas por episodio\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.show()\n",
    "\n",
    "plot_path = \"./test_rewards_plot.png\"\n",
    "plt.savefig(plot_path)\n",
    "\n",
    "\n",
    "wandb.log({\"Testing of Actor-Critic\": wandb.Image(plot_path)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
