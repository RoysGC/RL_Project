{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "from ale_py import ALEInterface\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "from utils_dueling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = filter out INFO, 2 = filter out WARNING, 3 = filter out ERROR\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "gym_logger = logging.getLogger('gym')\n",
    "gym_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API KEY: 73c9a156b91f3e0c01c3d5f332d23bfc66f4cdbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "#wandb.init(project=\"DuelingDQN\", entity = \"rl_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress TensorFlow logging\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)  # Seed for NumPy random number generator\n",
    "tf.random.set_seed(42)  # Seed for TensorFlow random number generator\n",
    "\n",
    "# Initialize ALEInterface (Arcade Learning Environment)\n",
    "ale = ALEInterface()\n",
    "\n",
    "# Configuration settings\n",
    "config = Config()\n",
    "best_reward = 0  # Variable to track the best reward achieved\n",
    "\n",
    "# Create the environment for the Assault game with RGB array rendering\n",
    "env = gym.make(\"Assault-v4\", render_mode=\"rgb_array\")\n",
    "n_actions = env.action_space.n  # Number of possible actions in the environment\n",
    "\n",
    "# Initialize Dueling DQN models for training and target\n",
    "model = DuelingDQN(n_actions)\n",
    "model_target = DuelingDQN(n_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "memory = ReplayMemory(config.MEMORY_SIZE)\n",
    "\n",
    "# Set up the optimizer and loss function for training\n",
    "optimizer = keras.optimizers.Adam(learning_rate=config.LEARNING_RATE, clipnorm=1.0)\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "# Lists to store episode rewards and losses\n",
    "episode_rewards = []\n",
    "losses = []\n",
    "\n",
    "best_reward = 0\n",
    "best_episode = 0\n",
    "best_frames = []\n",
    "\n",
    "epsilon = config.EPSILON_START  # Initial epsilon value for epsilon-greedy strategy\n",
    "\n",
    "# Training loop\n",
    "for episode in range(config.EPISODES):\n",
    "    state, info = env.reset()\n",
    "    state = state / 255.0  # Normalize state\n",
    "    done = False  # Boolean to track if the episode is done\n",
    "    episode_reward = 0  # Reward accumulated in the episode\n",
    "    steps = 0  # Step counter\n",
    "    info = {'lives': 4, 'episode_frame_number': 2, 'frame_number': 2}  # Info dictionary\n",
    "    frames = []  # List to store frames for GIF\n",
    "\n",
    "    while not done and info.get(\"lives\") > 0:  # Loop until the episode is done or lives are exhausted\n",
    "        action = take_action(state, epsilon, env, model)  # Take an action based on the current state\n",
    "        step_result = env.step(action)  # Step the environment with the selected action\n",
    "        next_state, reward, done, _, info = step_result[:5]  # Unpack the result\n",
    "        next_state = next_state / 255.0  # Normalize the next state\n",
    "\n",
    "        # Store the experience in replay memory\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        # Optimize models based on the stored experience\n",
    "        optimize_model(memory, config, model, model_target, n_actions, loss_function, optimizer)\n",
    "\n",
    "        # Render the environment and store the frame\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        # Update state and episode reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Print step details if reward is non-zero\n",
    "        if reward != 0:\n",
    "            print(\"step: \", steps, \"action: \", action, \" reward: \", reward)\n",
    "            print(\"Lives: \", info.get(\"lives\"))\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    # Print episode summary\n",
    "    print(f\"\\nEpisodio: {episode+1}, Recompensa: {episode_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    # Check if the episode reward is the best so far and save models if so\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward\n",
    "        # Ensure the model is built\n",
    "        if not model.built:\n",
    "            dummy_input = np.zeros((1, *env.observation_space.shape))\n",
    "            model(dummy_input)\n",
    "        # Save the model\n",
    "        model_save_path = f\"./dueling_model\"\n",
    "        model.save(model_save_path)\n",
    "        print(f\"Model saved\")\n",
    "\n",
    "        # Save the frames as a GIF\n",
    "        gif_path = f\"./episode_{episode+1}_reward_{episode_reward}.gif\"\n",
    "        imageio.mimsave(gif_path, frames, format='GIF', fps=30)\n",
    "\n",
    "    # Update epsilon for the epsilon-greedy strategy\n",
    "    epsilon = max((epsilon * config.EPSILON_DECAY_RATE), config.EPSILON_END)\n",
    "\n",
    "    # Log episode metrics to wandb (Weights & Biases)\n",
    "    wandb.log({\"episode\": episode + 1, \"reward\": episode_reward, \"epsilon\": epsilon})\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    # Update the target model weights\n",
    "    if (episode + 1) % config.TARGET_UPDATE == 0:\n",
    "        model_target.set_weights(model.get_weights())\n",
    "\n",
    "# Close the environment after training\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(episode_rewards, episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved Dueling DQN model\n",
    "dueling_model = keras.models.load_model(f\"./dueling_model\")\n",
    "\n",
    "# Create the Assault-v4 environment with RGB array rendering\n",
    "env = gym.make(\"Assault-v4\", render_mode = \"rgb_array\")\n",
    "n_actions = env.action_space.n  # Number of possible actions in the environment\n",
    "\n",
    "# List to store rewards for each episode\n",
    "rewards_per_episode = []\n",
    "best_reward = 0  # Variable to track the best reward achieved\n",
    "\n",
    "# Testing loop for 500 episodes\n",
    "for episode in range(500):\n",
    "    state, info = env.reset()\n",
    "    state = state / 255.0  # Normalize the initial state\n",
    "    done = False  # Boolean to track if the episode is done\n",
    "    episode_reward = 0  # Reward accumulated in the episode\n",
    "    frames = []  # List to store frames for GIF\n",
    "    info = {'lives': 4, 'episode_frame_number': 2, 'frame_number': 2}  # Info dictionary\n",
    "\n",
    "    while not done:\n",
    "        # Select action based on the model's prediction\n",
    "        action = np.argmax(dueling_model.predict(state[np.newaxis, ...]))\n",
    "        step_result = env.step(action)  # Step the environment with the selected action\n",
    "        next_state, reward, done, _, info = step_result[:5]  # Unpack the result\n",
    "        next_state = next_state / 255.0  # Normalize the next state\n",
    "\n",
    "        # Render the environment and store the frame\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        # Update state and episode reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Print action and reward details if reward is non-zero\n",
    "        if reward != 0:\n",
    "            print(\"action: \", action, \" reward: \", reward)\n",
    "            print(\"Lives: \", info.get(\"lives\"))\n",
    "\n",
    "    rewards_per_episode.append(episode_reward)  # Append the episode reward to the list\n",
    "    print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    # Save the best performing episode as a GIF\n",
    "    if episode_reward > best_reward:\n",
    "        best_reward = episode_reward \n",
    "        gif_path = f\"./test_episode_{episode+1}_reward_{episode_reward}.gif\"\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "\n",
    "    # Uncomment to log episode metrics and GIF to wandb\n",
    "    #wandb.log({\"episode\": episode + 1, \"reward\": episode_reward, \"epsilon\": epsilon, \"episode_gif\": wandb.Video(gif_path, fps=4, format=\"gif\")})\n",
    "\n",
    "# Close the environment after testing\n",
    "env.close()\n",
    "\n",
    "# Plot the rewards per episode\n",
    "plt.plot(rewards_per_episode)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Episode')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
