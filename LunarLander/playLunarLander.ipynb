{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import cv2\n",
    "from utils_lunarlander import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "#wandb.init(project=\"lunarLander_gif_26_11\", entity=\"rl_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up constants and configurations\n",
    "DEVICE = \"cpu\"  # Device configuration (CPU in this case)\n",
    "ACTION_SPACE = [0, 1, 2, 3]  # Defined action space for the LunarLander environment\n",
    "EPISODES = 100  # Total number of episodes for evaluation\n",
    "STEPS = 200  # Maximum number of steps per episode\n",
    "RENDER = True  # Boolean for rendering the environment\n",
    "\n",
    "# Defining the neural network for the reinforcement learning agent\n",
    "class ReinforceNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(ReinforceNetwork, self).__init__()\n",
    "        # Defining layers of the network\n",
    "        self.fc1 = nn.Linear(n_inputs, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = x.unsqueeze(0) if x.dim() == 1 else x\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        actions = torch.softmax(self.fc3(x), dim=-1)\n",
    "        action = self.get_action(actions)\n",
    "        log_prob_action = torch.log(actions.squeeze(0))[action]\n",
    "        return action, log_prob_action\n",
    "\n",
    "    def get_action(self, actions):\n",
    "        # Selecting an action based on the policy's output\n",
    "        return np.random.choice(ACTION_SPACE, p=actions.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "# Load the trained model\n",
    "model = ReinforceNetwork(8, 4).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"/workspaces/RL_Project/LunarLander/outputs/best_parameters_scheduler.pth\"))\n",
    "model.eval()  # Setting the model to evaluation mode\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "print(env.action_space, env.observation_space)\n",
    "fig = plt.figure()\n",
    "ims = []\n",
    "total_rewards_all_episodes = []  # Store total rewards for each episode\n",
    "\n",
    "# Evaluating the model over multiple episodes\n",
    "for episode in range(EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Iterate over steps in each episode\n",
    "    for step in range(STEPS):\n",
    "        img = env.render()  # Render the environment and capture the image\n",
    "        action, log_prob = model(state)\n",
    "        state, reward, done, info = env.step(action)[:4]\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # Visualize and store the image with overlayed text\n",
    "        cv2_im_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        pil_im = Image.fromarray(cv2_im_rgb)\n",
    "        draw = ImageDraw.Draw(pil_im)\n",
    "        draw.text((0, 0), f\"Step: {step} Action: {action} Reward: {int(reward)} Total Rewards: {int(np.sum(episode_rewards))} done: {done}\", fill=\"#FDFEFE\")\n",
    "        img = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)\n",
    "        im = plt.imshow(img, animated=True)\n",
    "        ims.append([im])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    total_rewards_all_episodes.append(np.sum(episode_rewards))\n",
    "\n",
    "# Close the environment post evaluation\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test(total_rewards_all_episodes, fig , ims )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
