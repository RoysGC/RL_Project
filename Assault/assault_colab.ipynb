{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages, 1 = filter out INFO, 2 = filter out WARNING, 3 = filter out ERROR\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import logging\n",
    "gym_logger = logging.getLogger('gym')\n",
    "gym_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"assault_paula_colab3\", entity = \"rl_proj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "from ale_py import ALEInterface\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ale = ALEInterface()\n",
    "\n",
    "# Inicialización del entorno\n",
    "#env = gym.make(\"Assault-v0\", render_mode=\"rgb_array\")\n",
    "env = gym.make(\"Assault-v4\", render_mode=\"rgb_array\") #PROVAR AQUESTA VERSIO DEL MODEL\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Parámetros modificados para una mayor exploración inicial y un decaimiento más lento\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "#EPSILON_DECAY = 10000  \n",
    "EPSILON_DECAY = 30000\n",
    "epsilon_decay_rate = 0.99\n",
    "EPISODES = 10 \n",
    "TARGET_UPDATE = 5\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "MAX_STEPS_PER_EPISODE = 1000\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(self.transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(keras.Model):\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = layers.Conv2D(16, 5, strides=2, activation=\"relu\")\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.layer2 = layers.Conv2D(16, 5, strides=2, activation=\"relu\")\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.layer3 = layers.Conv2D(32, 5, strides=2, activation=\"relu\")\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.layer4 = layers.Dense(512, activation=\"relu\")\n",
    "        self.action = layers.Dense(n_actions, activation=\"linear\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer4(x)\n",
    "        return self.action(x)\n",
    "\n",
    "# Creación del modelo y la memoria\n",
    "model = DQN(n_actions)\n",
    "model_target = DQN(n_actions)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Preparación del optimizador y la función de pérdida\n",
    "optimizer = keras.optimizers.Adam(learning_rate=2.5e-4, clipnorm=1.0)\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "def take_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        q_values = model.predict(state[np.newaxis, ...])\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "def optimize_model():\n",
    "    if memory.__len__() < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = memory.transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = np.array(batch.state)\n",
    "    action_batch = np.array(batch.action)\n",
    "    next_state_batch = np.array(batch.next_state)\n",
    "    rewad_batch = np.array(batch.reward)\n",
    "    done_batch = np.array(batch.done, dtype=np.int8)\n",
    "\n",
    "    future_rewards = model_target(next_state_batch)\n",
    "    target = rewad_batch + GAMMA * tf.reduce_max(future_rewards, axis=-1) * (1 - done_batch)\n",
    "\n",
    "    action_mask = tf.one_hot(action_batch, n_actions)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(state_batch)\n",
    "        q_action = tf.reduce_sum(tf.multiply(q_values, action_mask), axis=-1)\n",
    "        loss = loss_function(target, q_action)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    #model.save_weights('/Users/roy/Desktop/UNI')\n",
    "\n",
    "episode_rewards = []\n",
    "losses = []\n",
    "\n",
    "best_reward = -float(\"inf\")\n",
    "best_episode = 0\n",
    "best_frames = []\n",
    "\n",
    "# Entrenamiento del agente\n",
    "epsilon = EPSILON_START\n",
    "for episode in range(EPISODES):\n",
    "    #state, info = env.reset()  \n",
    "    #state = state / 255.0\n",
    "    state = env.reset() / 255.0\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    info ={'lives': 4, 'episode_frame_number': 2, 'frame_number': 2}\n",
    "\n",
    "    frames = []\n",
    "    #current_frames = []  # Almacena los frames del episodio actual\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done and steps < MAX_STEPS_PER_EPISODE and info.get(\"lives\") >= 0:\n",
    "        frame = env.render(\"rgb_array\")\n",
    "        frames.append(frame)\n",
    "        #ESTO ES NUEVO- REVISAR\n",
    "        #current_frames.append(frame)\n",
    "        # Comprobación y actualización de la mejor recompensa\n",
    "        \n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_episode = episode\n",
    "            #best_frames = current_frames  \n",
    "\n",
    "        action = take_action(state, epsilon)\n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, info = step_result[:5]\n",
    "        next_state = next_state / 255.0\n",
    "        #print(\"info: \", info)\n",
    "\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        optimize_model()\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if reward != 0:\n",
    "            print(\"step: \", steps, \"action: \", action, \" reward: \", reward)\n",
    "            print(\"Lives: \", info.get(\"lives\"))\n",
    "\n",
    "        steps += 1\n",
    "        \n",
    "        #epsilon = max(epsilon - (EPSILON_START - EPSILON_END) / EPSILON_DECAY, EPSILON_END)\n",
    "        epsilon = max(epsilon * epsilon_decay_rate, EPSILON_END)\n",
    "        \n",
    "        \n",
    "    print(f\"\\nEpisodio: {episode+1}, Recompensa: {episode_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    #gif_path = f\"episode_{episode+1}.gif\"\n",
    "    #gif_path = f\"/workspaces/RL_Project/Assault_gifs/episode_{episode+1}.gif\"\n",
    "    \n",
    "    gif_path = f\"/workspaces/RL_Project/Assault_gifs/episode_{best_episode+1}_reward_{episode_reward}.gif\"\n",
    "    imageio.mimsave(gif_path, frames, format='GIF', fps=30)\n",
    "    \n",
    "    wandb.log({\"episode\": episode + 1, \"reward\": episode_reward, \"epsilon\": epsilon})\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    if (episode + 1) % TARGET_UPDATE == 0:\n",
    "        model_target.set_weights(model.get_weights())\n",
    "\n",
    "    # Al final de cada episodio, graficar y guardar las métricas\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(episode_rewards, label=\"Recompensas por episodio\")\n",
    "    plt.xlabel(\"Episodio\")\n",
    "    plt.ylabel(\"Recompensa\")\n",
    "    plt.title(\"Evolución de la Recompensa por Episodio\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"/content/assault_plots/reward_plot_episode_{episode+1}.png\")\n",
    "    wandb.log({\"reward_plot\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best episode: \", best_episode+1)\n",
    "\n",
    "print(\"best reward: \", best_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
