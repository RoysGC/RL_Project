{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nenv = gym.make('LunarLander-v2')\\n\\n# Ahora puedes interactuar con el entorno\\nobs = env.reset()\\nfor _ in range(1000):\\n    env.render()\\n    action = env.action_space.sample() # Elegir una acción al azar\\n    obs, reward, done, info = env.step(action)\\n    if done:\\n        obs = env.reset()\\nenv.close()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\"\"\"\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Ahora puedes interactuar con el entorno\n",
    "obs = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # Elegir una acción al azar\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "env.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make (\"LunarLander-v2\", continuous = False )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class PolicyNetwork(nn.Module):\\n    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\\n        super(PolicyNetwork, self).__init__()\\n        self.fc1 = nn.Linear(input_size, hidden_size1)\\n        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\\n        self.fc3 = nn.Linear(hidden_size2, output_size)\\n\\n    def forward(self, x):\\n        x = F.tanh(self.fc1(x))\\n        x = F.tanh(self.fc2(x))\\n        x = F.softmax(self.fc3(x), dim=-1)\\n        return x'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=-1)\n",
    "        return x\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return F.softmax(self.fc3(x), dim=-1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes=500):\n",
    "    # Hiperparámetros\n",
    "    learning_rate = 0.005\n",
    "    gamma = 0.99\n",
    "\n",
    "    # Inicialización del entorno y la red\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    state, _ = env.reset()  # Extraer solo el array de NumPy\n",
    "    policy_net = PolicyNetwork(state.shape[0], 16, 32, env.action_space.n)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()  # Extraer solo el array de NumPy\n",
    "        #print(\"reset hecho\")\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        done = False\n",
    "        #print(\"variables inicializadas\")\n",
    "        while not done:\n",
    "            #print(\"entrada en bucle\")\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            #print(\"state_tensor creado\")\n",
    "            probs = policy_net(state_tensor)\n",
    "            #print(\"probs hechas con policy network\")\n",
    "            action = torch.multinomial(probs, 1).item()\n",
    "            #print(\"action seleccionada\")\n",
    "            next_state = env.step(action)  # Manejar la respuesta completa\n",
    "            #print(\"next state hecho\")\n",
    "\n",
    "            rewards.append(next_state[1])\n",
    "            #print(\"se añade reward a la lista\")\n",
    "            log_prob = torch.log(probs.squeeze(0)[action])\n",
    "            #print(\"log prob hecho\")\n",
    "            log_probs.append(log_prob)\n",
    "            #print(\"log prob añadido a la lista\")\n",
    "\n",
    "            state = next_state[0]\n",
    "            #print(\"actualización del state\")\n",
    "\n",
    "            done = next_state[2]\n",
    "\n",
    "        # Calcular retorno con descuento\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5) # Normalizar\n",
    "        #print(\"calculo return\")\n",
    "        episode_total_reward = sum(rewards)\n",
    "        episode_rewards.append(episode_total_reward)\n",
    "        \n",
    "        # Actualizar la política\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat([loss.unsqueeze(0) for loss in policy_loss]).sum()\n",
    "        #policy_loss = torch.cat(policy_loss).sum()\n",
    "        losses.append(policy_loss.item())\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"actualizacion de la politica\")\n",
    "\n",
    "    # Guardar el modelo\n",
    "    torch.save(policy_net.state_dict(), 'lunar_lander_policy.pth')\n",
    "    env.close()\n",
    "    #print(\"guardar el modelo y cerrar entorno\")\n",
    "\n",
    "    # Graficar recompensas y pérdidas\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.title('Recompensas por Episodio')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa Total')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(losses)\n",
    "    plt.title('Pérdida de la Política por Episodio')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #print(\"plotear los graficos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de 'state' después de env.reset(): <class 'tuple'>\n",
      "Contenido de 'state' después de env.reset(): (array([ 0.00689821,  1.418259  ,  0.6986969 ,  0.3261522 , -0.00798648,\n",
      "       -0.15826517,  0.        ,  0.        ], dtype=float32), {})\n",
      "Tipo de 'next_state' después de env.step(action): <class 'tuple'>\n",
      "Contenido de 'next_state' después de env.step(action): (array([ 0.01372433,  1.4250116 ,  0.688685  ,  0.30006975, -0.01398962,\n",
      "       -0.12007264,  0.        ,  0.        ], dtype=float32), 0.6749093240617594, False, False, {})\n",
      "0.6749093240617594\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "state = env.reset()\n",
    "\n",
    "print(\"Tipo de 'state' después de env.reset():\", type(state))\n",
    "print(\"Contenido de 'state' después de env.reset():\", state)\n",
    "\n",
    "action = env.action_space.sample()  # Tomar una acción aleatoria\n",
    "next_state = env.step(action)\n",
    "\n",
    "print(\"Tipo de 'next_state' después de env.step(action):\", type(next_state))\n",
    "print(\"Contenido de 'next_state' después de env.step(action):\", next_state)\n",
    "print(next_state[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
